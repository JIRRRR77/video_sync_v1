{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba03dbf",
   "metadata": {},
   "source": [
    "## MultiHeadAttention using attention matrix for delay estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84ca5cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models, regularizers\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "732d8443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_features: (1, 20, 48)\n"
     ]
    }
   ],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self, type=\"flows\", l2_reg=0.01):\n",
    "        super(CNN, self).__init__()\n",
    "        self.type = type\n",
    "        if type == \"images_flows\":\n",
    "            dim = 6\n",
    "        elif type == \"cross_flows\":\n",
    "            dim = 9\n",
    "        else:\n",
    "            dim = 3\n",
    "        self.cnn = models.Sequential()\n",
    "\n",
    "        reg = regularizers.l2(l2_reg)\n",
    "\n",
    "        self.cnn.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, dim), kernel_regularizer=reg))\n",
    "        self.cnn.add(layers.MaxPooling2D((3, 3)))\n",
    "\n",
    "        self.cnn.add(layers.Conv2D(48, (3, 3), activation='relu', kernel_regularizer=reg))\n",
    "        self.cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "        self.cnn.add(layers.Conv2D(48, (3, 3), activation='relu', kernel_regularizer=reg))\n",
    "        self.cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "        self.cnn.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=reg))\n",
    "        self.cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "        self.cnn.add(layers.GlobalAveragePooling2D())\n",
    "        self.cnn.add(layers.Dense(48, activation='relu', kernel_regularizer=reg))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.cnn(inputs)\n",
    "    \n",
    "Extractor = CNN()\n",
    "\n",
    "random_images = np.random.random((20, 224, 224, 3)) * 255\n",
    "random_images = random_images.astype('float32')/ 255.0\n",
    "left_imgs = tf.convert_to_tensor(random_images, dtype=tf.float32)\n",
    "\n",
    "random_images = np.random.random((20, 224, 224, 3)) * 255\n",
    "random_images = random_images.astype('float32')/ 255.0\n",
    "right_imgs = tf.convert_to_tensor(random_images, dtype=tf.float32)\n",
    "\n",
    "left_features = Extractor(left_imgs)\n",
    "right_features = Extractor(right_imgs)\n",
    "left_features = tf.expand_dims(left_features,axis=0)  #[batch_size, query_seq_length, feature_dim]\n",
    "right_features = tf.expand_dims(right_features,axis=0) # [batch_size, key_seq_length, feature_dim]\n",
    "\n",
    "print(\"left_features:\",left_features.shape) #(1, 20, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "326f9aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8, 20, 20)\n",
      "(1, 20, 20)\n",
      "-17\n"
     ]
    }
   ],
   "source": [
    "# 调用 MultiHeadAttention 层，获取输出和注意力权重\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "\n",
    "query, key, value = left_features,right_features,right_features\n",
    "attention_layer = MultiHeadAttention(num_heads=8, key_dim=64)\n",
    "output, attention_scores = attention_layer(query, key, value, return_attention_scores=True)\n",
    "similarity_matrix = tf.reduce_mean(attention_scores,axis=1)\n",
    "\n",
    "print(attention_scores.shape)   # (1, 8, 20, 20)\n",
    "print(similarity_matrix.shape)  # (1, 20, 20)\n",
    "\n",
    "matrix = similarity_matrix[0]\n",
    "\n",
    "# 计算对角线的均值并找到均值最大的对角线\n",
    "diagonal_means = []\n",
    "for offset in range(-19, 20):\n",
    "    diagonal = np.diagonal(matrix, offset=offset)\n",
    "    if diagonal.size > 0:\n",
    "        mean_value = np.mean(diagonal)\n",
    "        diagonal_means.append((offset, mean_value))\n",
    "\n",
    "# 找到均值最大的对角线\n",
    "max_mean = max(diagonal_means, key=lambda x: x[1])\n",
    "max_offset, _ = max_mean\n",
    "\n",
    "print(max_offset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "250b0793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制热图\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(matrix, annot=False, cmap='coolwarm', cbar=True)\n",
    "\n",
    "\n",
    "# 定义红色方框的样式\n",
    "box_color = 'yellow'\n",
    "box_linewidth = 4\n",
    "\n",
    "# 添加红色方框来高亮显示均值最大的对角线\n",
    "if max_offset >= 0:\n",
    "    for i in range(min(20 - max_offset, 20)):\n",
    "        ax.add_patch(Rectangle((i, i + max_offset), 1, 1, fill=False, edgecolor=box_color, lw=box_linewidth))\n",
    "else:\n",
    "    for i in range(min(20, 20 + max_offset)):\n",
    "        ax.add_patch(Rectangle((i - max_offset, i), 1, 1, fill=False, edgecolor=box_color, lw=box_linewidth))\n",
    "\n",
    "plt.title('20x20 Similarity Matrix with Highlighted Diagonal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53147fcd",
   "metadata": {},
   "source": [
    "## Seem like if the output is attention_scores,the gradient can not be back propagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5ef7fe9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_32\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_100 (InputLayer)         [(None, 20, 64)]     0           []                               \n",
      "                                                                                                  \n",
      " input_101 (InputLayer)         [(None, 20, 64)]     0           []                               \n",
      "                                                                                                  \n",
      " input_102 (InputLayer)         [(None, 20, 64)]     0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention_79 (Multi  ((None, 20, 64),    132672      ['input_100[0][0]',              \n",
      " HeadAttention)                  (None, 8, 20, 20))               'input_101[0][0]',              \n",
      "                                                                  'input_102[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 132,672\n",
      "Trainable params: 132,672\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['multi_head_attention_79/value/kernel:0', 'multi_head_attention_79/value/bias:0', 'multi_head_attention_79/attention_output/kernel:0', 'multi_head_attention_79/attention_output/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['multi_head_attention_79/value/kernel:0', 'multi_head_attention_79/value/bias:0', 'multi_head_attention_79/attention_output/kernel:0', 'multi_head_attention_79/attention_output/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "1/1 [==============================] - 1s 1s/step - loss: 811.7873\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 811.7873\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 811.7873\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 811.7873\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 811.7873\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 811.7872\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 811.7873\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 811.7873\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 811.7873\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 811.7874\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MultiHeadAttention, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "class MultiHeadAttentionModel:\n",
    "    def __init__(self, num_heads, key_dim, sequence_length, input_dim):\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        query_input = Input(shape=(self.sequence_length, self.input_dim))\n",
    "        key_input = Input(shape=(self.sequence_length, self.input_dim))\n",
    "        value_input = Input(shape=(self.sequence_length, self.input_dim))\n",
    "\n",
    "        # MultiHeadAttention\n",
    "        attention_layer = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)\n",
    "        attention_output, attention_scores = attention_layer(query=query_input, key=key_input, value=value_input, return_attention_scores=True)\n",
    "\n",
    "        model = Model(inputs=[query_input, key_input, value_input], outputs=attention_scores)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def compile_model(self, optimizer='adam'):\n",
    "        self.model.compile(optimizer=optimizer, loss=self.loss_diagonal)\n",
    "    \n",
    "#     def loss(self, y_true, attention_scores):\n",
    "#         return tf.reduce_mean(tf.square(y_true - attention_scores))\n",
    "    \n",
    "    def loss_diagonal(self, y_true, attention_scores):\n",
    "        similarity_matrix = tf.reduce_mean(attention_scores, axis=1)\n",
    "\n",
    "        def calculate_max_offset(matrix):\n",
    "            diagonal_means = []\n",
    "            for offset in range(-self.sequence_length + 1, self.sequence_length):\n",
    "                diagonal = tf.linalg.diag_part(matrix, k=offset)\n",
    "                mean_value = tf.cond(tf.size(diagonal) > 0, lambda: tf.reduce_mean(diagonal), lambda: tf.constant(0.0))\n",
    "                diagonal_means.append(mean_value)\n",
    "            diagonal_means = tf.stack(diagonal_means)\n",
    "            max_offset = tf.argmax(diagonal_means)\n",
    "            return tf.cast(max_offset, tf.float32)\n",
    "\n",
    "        max_offsets = tf.map_fn(calculate_max_offset, similarity_matrix, fn_output_signature=tf.float32)\n",
    "\n",
    "        return tf.reduce_mean(tf.square(y_true - max_offsets))\n",
    "        \n",
    "    def train_model(self, X_train, y_train, epochs=10, batch_size=32):\n",
    "        self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_heads = 8\n",
    "    key_dim = 64\n",
    "    sequence_length = 20\n",
    "    input_dim = 48\n",
    "\n",
    "    mha_model = MultiHeadAttentionModel(num_heads, key_dim, sequence_length, input_dim)\n",
    "    mha_model.compile_model()\n",
    "\n",
    "    mha_model.summary()\n",
    "\n",
    "    query_data = np.random.random((32, sequence_length, input_dim))\n",
    "    key_data = np.random.random((32, sequence_length, input_dim))\n",
    "    value_data = key_data\n",
    "    y_train = np.random.random((32,))\n",
    "#     y_train = np.random.random((32,8,20,20))\n",
    "\n",
    "    X_train = [query_data, key_data, value_data]\n",
    "    mha_model.train_model(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b57ac6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['custom_multi_head_attention_6/value/kernel:0', 'custom_multi_head_attention_6/value/bias:0', 'custom_multi_head_attention_6/attention_output/kernel:0', 'custom_multi_head_attention_6/attention_output/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['custom_multi_head_attention_6/value/kernel:0', 'custom_multi_head_attention_6/value/bias:0', 'custom_multi_head_attention_6/attention_output/kernel:0', 'custom_multi_head_attention_6/attention_output/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 4.0536\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 4.0681\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 4.1795\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 4.6647\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 5.7864\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 7.8977\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 10.9386\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 15.0640\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 19.9778\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 25.8761\n",
      "Gradients on trainable variables: [<tf.Tensor: shape=(64, 8, 64), dtype=float32, numpy=\n",
      "array([[[  9.172353 ,  -9.714582 ,   9.71274  , ...,  -9.495641 ,\n",
      "          -9.7160225,  -9.482173 ],\n",
      "        [  8.59832  ,   9.238859 ,   8.429424 , ...,  -9.098757 ,\n",
      "           8.743565 ,  -8.9342165],\n",
      "        [ -9.475843 ,   9.035646 ,  -9.460613 , ...,  -9.279517 ,\n",
      "          -9.372499 ,   9.686691 ],\n",
      "        ...,\n",
      "        [  8.849537 ,   9.510788 ,  -9.016194 , ...,   8.950693 ,\n",
      "           9.383951 ,  -8.905172 ],\n",
      "        [ -9.048435 ,  -8.898557 ,  -9.254261 , ...,   8.943758 ,\n",
      "          -8.235509 ,   9.06795  ],\n",
      "        [  9.324778 ,   9.0289135,   9.012803 , ...,  -9.363837 ,\n",
      "           8.897177 ,  -9.526165 ]],\n",
      "\n",
      "       [[  9.544006 , -10.250765 ,  10.13748  , ...,  -9.923855 ,\n",
      "         -10.194573 ,  -9.9186325],\n",
      "        [  8.782077 ,   9.721729 ,   8.793289 , ...,  -9.435999 ,\n",
      "           9.073544 ,  -9.223805 ],\n",
      "        [ -9.8453665,   9.300442 ,  -9.856297 , ...,  -9.561069 ,\n",
      "          -9.73836  ,  10.143772 ],\n",
      "        ...,\n",
      "        [  9.028574 ,   9.9396305,  -9.28673  , ...,   9.397238 ,\n",
      "           9.773182 ,  -9.028759 ],\n",
      "        [ -9.362338 ,  -9.262138 ,  -9.708727 , ...,   9.372531 ,\n",
      "          -8.591478 ,   9.331713 ],\n",
      "        [  9.541689 ,   9.292494 ,   9.362644 , ...,  -9.543483 ,\n",
      "           9.134956 ,  -9.790613 ]],\n",
      "\n",
      "       [[  9.287369 , -10.08587  ,   9.923819 , ...,  -9.497904 ,\n",
      "          -9.848122 ,  -9.607221 ],\n",
      "        [  8.836711 ,   9.396339 ,   8.744772 , ...,  -9.51885  ,\n",
      "           9.154521 ,  -9.2540045],\n",
      "        [ -9.6451235,   9.053379 ,  -9.411491 , ...,  -9.321585 ,\n",
      "          -9.419237 ,   9.819244 ],\n",
      "        ...,\n",
      "        [  9.011943 ,   9.666304 ,  -8.970848 , ...,   9.116278 ,\n",
      "           9.43363  ,  -8.938046 ],\n",
      "        [ -9.320524 ,  -9.006304 ,  -9.712938 , ...,   9.346777 ,\n",
      "          -8.440726 ,   9.188037 ],\n",
      "        [  9.360913 ,   8.943277 ,   9.224703 , ...,  -9.240995 ,\n",
      "           8.991557 ,  -9.633686 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[  9.131935 ,  -9.754661 ,   9.7246685, ...,  -9.513029 ,\n",
      "          -9.765263 ,  -9.593702 ],\n",
      "        [  8.312449 ,   8.963156 ,   8.341617 , ...,  -8.938793 ,\n",
      "           8.614265 ,  -8.627722 ],\n",
      "        [ -9.465654 ,   8.8684225,  -9.387779 , ...,  -9.08714  ,\n",
      "          -9.346765 ,   9.6050205],\n",
      "        ...,\n",
      "        [  8.674922 ,   9.2578745,  -8.6644125, ...,   8.842497 ,\n",
      "           9.262866 ,  -8.448961 ],\n",
      "        [ -9.083906 ,  -9.020401 ,  -9.49395  , ...,   9.079949 ,\n",
      "          -8.3014   ,   9.174984 ],\n",
      "        [  9.265425 ,   9.060493 ,   9.144143 , ...,  -9.277322 ,\n",
      "           9.084449 ,  -9.540432 ]],\n",
      "\n",
      "       [[  9.327664 , -10.075588 ,   9.951533 , ...,  -9.687685 ,\n",
      "          -9.903601 ,  -9.749273 ],\n",
      "        [  8.815587 ,   9.719017 ,   8.754019 , ...,  -9.5291605,\n",
      "           9.103767 ,  -9.254157 ],\n",
      "        [ -9.445428 ,   8.978556 ,  -9.442717 , ...,  -9.20784  ,\n",
      "          -9.366351 ,   9.678919 ],\n",
      "        ...,\n",
      "        [  8.887932 ,   9.665095 ,  -8.968079 , ...,   9.068134 ,\n",
      "           9.5350895,  -8.86871  ],\n",
      "        [ -9.003807 ,  -8.803413 ,  -9.272034 , ...,   8.937099 ,\n",
      "          -8.250322 ,   8.961578 ],\n",
      "        [  9.299471 ,   9.044857 ,   9.119708 , ...,  -9.283428 ,\n",
      "           8.993948 ,  -9.501663 ]],\n",
      "\n",
      "       [[  8.574841 ,  -9.163302 ,   9.01676  , ...,  -8.955168 ,\n",
      "          -9.141553 ,  -8.854395 ],\n",
      "        [  8.146751 ,   8.837697 ,   7.9485083, ...,  -8.663696 ,\n",
      "           8.279621 ,  -8.453627 ],\n",
      "        [ -8.810376 ,   8.423001 ,  -8.869864 , ...,  -8.5934105,\n",
      "          -8.763137 ,   9.122487 ],\n",
      "        ...,\n",
      "        [  8.269863 ,   8.910288 ,  -8.23481  , ...,   8.444667 ,\n",
      "           8.760845 ,  -8.234676 ],\n",
      "        [ -8.390188 ,  -8.290886 ,  -8.643984 , ...,   8.335695 ,\n",
      "          -7.7691374,   8.351669 ],\n",
      "        [  8.540017 ,   8.466505 ,   8.450521 , ...,  -8.606539 ,\n",
      "           8.265602 ,  -8.839221 ]]], dtype=float32)>, <tf.Tensor: shape=(8, 64), dtype=float32, numpy=\n",
      "array([[ 18.294487, -19.49549 ,  19.264103,  18.512253,  17.746126,\n",
      "        -19.151308,  17.859509, -17.910599,  17.69951 ,  18.45052 ,\n",
      "         18.155703, -17.877237,  17.571932,  17.706573,  18.339159,\n",
      "         17.460617,  19.330713, -19.133911,  18.755104,  18.72213 ,\n",
      "         18.28635 , -18.110062,  17.011333,  18.06199 ,  18.747673,\n",
      "         18.529442,  19.578835, -18.332426,  15.597047, -19.44987 ,\n",
      "         18.11175 ,  18.024334,  18.205795, -18.665184, -18.636866,\n",
      "        -18.625055, -18.01469 , -17.917013,  18.569162,  18.146301,\n",
      "         18.233011,  18.650616, -17.980967, -16.728535,  18.223812,\n",
      "         18.337212,  18.33993 ,  18.098223,  17.922714,  18.937561,\n",
      "         18.511082, -19.670073, -18.332106,  17.968933,  18.12017 ,\n",
      "         17.614048,  18.705725, -18.361053,  17.557331,  18.74954 ,\n",
      "        -18.47582 , -18.806831, -19.239494, -18.848604],\n",
      "       [ 16.979555,  18.520552,  16.764946, -17.98954 , -18.212172,\n",
      "        -16.329765,  18.002659, -17.850891,  17.83939 , -18.501122,\n",
      "         18.277164, -18.699991, -17.796854,  17.779547,  18.05883 ,\n",
      "         17.879557,  17.28012 ,  18.017614, -18.430832,  17.272923,\n",
      "         18.017498, -18.255861,  18.384283, -17.949509, -17.878756,\n",
      "         17.932735,  17.978907,  18.456394, -17.511463, -17.706612,\n",
      "         18.350735,  18.84837 , -17.286158, -17.14172 ,  16.6349  ,\n",
      "        -18.636965, -18.182898, -17.903446,  18.033861,  18.571075,\n",
      "        -17.59399 , -17.74048 , -18.870323, -18.530249, -16.647665,\n",
      "        -17.102287,  18.133244,  17.402012, -18.036428,  17.412125,\n",
      "        -18.132189,  17.03612 ,  16.933737,  17.895605, -16.978323,\n",
      "        -17.231722,  17.820377, -17.105576, -17.501318, -19.140402,\n",
      "        -18.220621, -18.117468,  17.53076 , -17.746773],\n",
      "       [-18.667604,  17.833652, -18.665855,  18.791384, -19.06116 ,\n",
      "        -18.333902, -18.184258,  18.580696,  17.947853, -19.087591,\n",
      "         18.402756, -18.281822,  18.676054,  19.14588 ,  18.466501,\n",
      "        -19.359867,  18.341389,  19.071033, -18.042215,  18.543358,\n",
      "         18.640371, -18.68735 ,  19.074654, -18.284792, -18.678219,\n",
      "         18.241377,  19.140749, -17.017675,  18.733746, -19.037369,\n",
      "         19.04878 , -19.465532, -17.49271 ,  18.678734,  18.144909,\n",
      "        -19.06403 ,  18.307913,  19.416983, -18.21499 , -17.153824,\n",
      "        -18.36412 ,  18.37919 ,  18.37305 , -19.114975, -17.568987,\n",
      "        -18.244156,  19.42754 ,  18.692387,  18.537985,  19.231295,\n",
      "         16.218592, -19.750708, -17.989618, -18.006002,  18.557667,\n",
      "         17.910683, -17.389761,  18.281342, -19.423836, -17.866701,\n",
      "         18.669104, -18.239021, -18.527037,  19.263638],\n",
      "       [-18.754686, -19.21527 , -18.084633, -19.072266, -17.948843,\n",
      "         18.906958,  19.52832 ,  18.526602,  17.014284,  18.35048 ,\n",
      "        -18.264017, -19.511047,  18.571972,  18.979599, -18.49103 ,\n",
      "        -17.753326, -18.070171, -18.752314,  18.412054, -19.114775,\n",
      "        -19.684048,  18.852678,  18.229156,  18.147337, -18.845673,\n",
      "        -18.574612,  18.227585, -19.262602,  18.379908, -18.506382,\n",
      "        -17.927761, -18.480333,  18.670403, -17.637255,  17.69956 ,\n",
      "         17.80601 ,  18.249716,  18.239264, -18.365862, -18.685844,\n",
      "         18.111927,  18.311779,  17.95022 , -17.801352, -18.523647,\n",
      "         17.963327, -18.21414 ,  17.751898, -19.12854 , -18.025536,\n",
      "         18.492184, -17.460196,  18.24089 ,  17.87735 ,  18.167757,\n",
      "         17.952576, -17.698275, -18.804705,  17.690926,  18.23337 ,\n",
      "        -18.11248 ,  18.130247, -18.964647, -17.34019 ],\n",
      "       [ 19.18902 , -18.410376, -19.900406,  18.909779,  18.298515,\n",
      "         18.98551 ,  17.861525,  19.152048,  18.438795,  20.153627,\n",
      "         19.302237,  19.146505, -18.950975, -17.899527,  19.925001,\n",
      "         18.328209,  18.739916, -18.939253,  18.169125,  19.031015,\n",
      "        -18.891577,  19.380062,  19.822102,  18.615053,  19.059797,\n",
      "        -19.07535 ,  19.296734, -19.418276, -18.26642 , -18.8915  ,\n",
      "        -18.416107,  19.158922, -18.812033, -18.618378,  18.527555,\n",
      "         19.397873,  18.161495, -19.149101, -19.26565 , -18.52935 ,\n",
      "        -19.3203  ,  19.426731,  19.1933  , -18.448492, -18.79998 ,\n",
      "         19.005732, -18.885704,  19.368538, -19.73073 , -18.738678,\n",
      "         18.692556, -19.240278, -16.412464,  19.474518,  17.35319 ,\n",
      "        -18.535295,  20.001715,  18.17447 ,  18.321451, -19.217241,\n",
      "         18.630417,  18.2545  , -18.843563,  19.202188],\n",
      "       [ 17.487621,  18.948301, -17.61581 , -17.59154 , -19.24453 ,\n",
      "        -17.633558, -18.805044, -17.361525, -18.04245 ,  18.408466,\n",
      "         17.827583,  18.503494, -18.019714, -17.748634,  18.977673,\n",
      "         18.802362,  17.6356  ,  18.683643,  18.669086, -18.339848,\n",
      "        -18.387022, -16.580368, -18.53476 , -17.811869,  18.908512,\n",
      "        -18.445127,  18.319038, -18.27626 , -17.75428 , -18.667288,\n",
      "        -17.703646, -17.779264, -18.858446,  18.191349, -18.228884,\n",
      "         16.821922,  18.751411, -18.531763,  18.34877 ,  17.568447,\n",
      "        -18.497292,  18.225191, -18.072355, -17.546125, -17.299349,\n",
      "        -18.769707, -17.678244,  18.137203,  18.07536 , -18.252537,\n",
      "         17.844845, -18.077047, -18.24011 ,  18.337132, -18.193636,\n",
      "        -18.496866,  18.045147,  18.779633,  17.995087, -18.730476,\n",
      "        -18.915123,  17.85577 ,  18.587278, -17.323292],\n",
      "       [-17.904902, -17.636889, -18.553003,  17.824736,  17.578182,\n",
      "        -18.030506, -17.710121,  16.984892,  18.531527, -18.921764,\n",
      "        -16.42722 , -19.093798, -18.322409,  18.503239, -18.06937 ,\n",
      "        -17.974632,  17.629778,  18.345608,  17.902351, -18.003994,\n",
      "         16.957363, -17.079891,  17.946795,  15.987861,  18.939081,\n",
      "        -18.113207,  16.836273, -18.340055, -17.9108  , -17.42853 ,\n",
      "        -18.394554, -17.085308,  17.44344 , -17.419456,  18.202913,\n",
      "        -17.826714,  17.903181,  18.219797,  18.042301, -17.339886,\n",
      "        -18.582474,  17.5378  , -18.001942, -18.357468, -18.114693,\n",
      "        -17.56236 , -18.199291,  18.60278 , -17.420528,  18.305828,\n",
      "        -17.383923,  17.850292,  18.413887,  17.981188,  18.087008,\n",
      "        -18.483217, -18.240324, -18.413841, -18.569529, -18.067612,\n",
      "        -16.804281,  17.948252, -16.417187,  17.8367  ],\n",
      "       [ 18.507175,  17.97716 ,  18.078983, -18.517237,  18.113987,\n",
      "         17.878735,  17.347153, -18.924465,  17.75071 , -18.108805,\n",
      "        -17.858185,  18.306358, -17.712286,  18.757092, -18.211355,\n",
      "         18.585474, -18.195143, -18.338068,  19.01386 , -16.441782,\n",
      "        -18.48544 ,  17.408545,  18.372889, -17.929537,  18.516722,\n",
      "        -19.292   , -18.843582,  18.218174,  18.182209,  18.265202,\n",
      "         18.003462,  18.133684, -18.630154, -19.408682, -17.825153,\n",
      "         18.729006, -17.541431,  18.333021, -18.161367,  17.189148,\n",
      "         19.047424,  17.58445 , -15.953717, -18.425602,  18.556715,\n",
      "        -17.817343,  17.35413 , -19.062569, -17.97342 , -18.190903,\n",
      "         18.387884,  18.540369,  17.572342, -17.298195,  17.431726,\n",
      "        -17.475838, -18.407963,  18.650976, -18.218712, -17.495129,\n",
      "        -18.111404, -18.559288,  17.797668, -19.065111]], dtype=float32)>, <tf.Tensor: shape=(64, 8, 64), dtype=float32, numpy=\n",
      "array([[[ -1.2906418 ,   1.2663784 ,  -1.2379198 , ...,   1.3104677 ,\n",
      "           1.3193884 ,   1.2317886 ],\n",
      "        [  0.96553993,   0.96704435,   0.9655824 , ...,  -0.9190483 ,\n",
      "           0.918313  ,  -0.9523411 ],\n",
      "        [  0.41781616,  -0.4063468 ,   0.50270987, ...,   0.42999077,\n",
      "           0.48674965,  -0.4800496 ],\n",
      "        ...,\n",
      "        [ -0.7198491 ,  -0.72041225,   0.7236614 , ...,  -0.7195072 ,\n",
      "          -0.7243824 ,   0.8147006 ],\n",
      "        [ -1.0149417 ,  -1.118104  ,  -1.0273881 , ...,   1.0099034 ,\n",
      "          -1.0962095 ,   1.07093   ],\n",
      "        [  2.7579222 ,   2.587163  ,   2.7071958 , ...,  -2.7078004 ,\n",
      "           2.659698  ,  -2.7191172 ]],\n",
      "\n",
      "       [[ -7.1438313 ,   7.221288  ,  -7.317454  , ...,   7.122105  ,\n",
      "           7.376238  ,   7.18111   ],\n",
      "        [ -9.369035  ,  -9.633545  ,  -9.422789  , ...,   9.392309  ,\n",
      "          -9.291107  ,   9.83066   ],\n",
      "        [ 10.409002  , -10.275709  ,  10.453683  , ...,  10.922968  ,\n",
      "          10.42716   , -10.641457  ],\n",
      "        ...,\n",
      "        [ -3.5983114 ,  -3.5193307 ,   3.680541  , ...,  -3.6099677 ,\n",
      "          -3.5022953 ,   3.6569223 ],\n",
      "        [  6.673739  ,   6.6811757 ,   6.6491423 , ...,  -6.692641  ,\n",
      "           6.4954953 ,  -6.8109045 ],\n",
      "        [ -8.217355  ,  -7.8448973 ,  -8.306839  , ...,   8.341499  ,\n",
      "          -8.046073  ,   8.137281  ]],\n",
      "\n",
      "       [[ 21.471996  , -21.916409  ,  21.54795   , ..., -21.593204  ,\n",
      "         -21.99739   , -21.76545   ],\n",
      "        [ 20.48716   ,  21.345934  ,  20.879396  , ..., -20.82997   ,\n",
      "          20.805458  , -21.647972  ],\n",
      "        [-25.014347  ,  24.605173  , -25.18641   , ..., -25.936325  ,\n",
      "         -24.715073  ,  25.264166  ],\n",
      "        ...,\n",
      "        [ 26.0459    ,  26.158384  , -26.329956  , ...,  26.427586  ,\n",
      "          25.93361   , -27.381256  ],\n",
      "        [-25.172352  , -25.451986  , -25.51363   , ...,  25.5397    ,\n",
      "         -24.612087  ,  26.074387  ],\n",
      "        [ 25.469082  ,  24.548887  ,  25.918663  , ..., -26.232405  ,\n",
      "          25.29813   , -25.559618  ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 18.070076  , -18.399765  ,  18.089449  , ..., -18.363781  ,\n",
      "         -18.69189   , -18.320902  ],\n",
      "        [ 16.31336   ,  17.168299  ,  16.660513  , ..., -16.744385  ,\n",
      "          16.576336  , -17.346037  ],\n",
      "        [-17.910177  ,  17.65319   , -17.984425  , ..., -18.525883  ,\n",
      "         -17.668407  ,  18.05296   ],\n",
      "        ...,\n",
      "        [ 18.070072  ,  18.092314  , -18.285103  , ...,  18.373096  ,\n",
      "          17.997814  , -18.972898  ],\n",
      "        [-16.368456  , -16.550236  , -16.408707  , ...,  16.562502  ,\n",
      "         -15.933088  ,  16.867725  ],\n",
      "        [ 17.071762  ,  16.543068  ,  17.456142  , ..., -17.73667   ,\n",
      "          17.159412  , -17.21874   ]],\n",
      "\n",
      "       [[ 21.64687   , -22.081406  ,  21.69849   , ..., -21.828537  ,\n",
      "         -22.301811  , -21.9585    ],\n",
      "        [ 21.633892  ,  22.670872  ,  22.063992  , ..., -22.000896  ,\n",
      "          21.81392   , -22.935608  ],\n",
      "        [-24.861237  ,  24.415112  , -24.826147  , ..., -25.735868  ,\n",
      "         -24.509516  ,  25.16516   ],\n",
      "        ...,\n",
      "        [ 23.953218  ,  24.002235  , -24.28563   , ...,  24.405722  ,\n",
      "          23.980434  , -24.996452  ],\n",
      "        [-26.436008  , -26.870777  , -26.724741  , ...,  26.88779   ,\n",
      "         -25.98784   ,  27.297516  ],\n",
      "        [ 24.601711  ,  23.659584  ,  25.025017  , ..., -25.348633  ,\n",
      "          24.463017  , -24.76318   ]],\n",
      "\n",
      "       [[  7.5931664 ,  -7.6880884 ,   7.6345077 , ...,  -7.595953  ,\n",
      "          -7.758542  ,  -7.678073  ],\n",
      "        [ 10.456533  ,  10.896446  ,  10.690959  , ..., -10.682657  ,\n",
      "          10.489641  , -11.094728  ],\n",
      "        [ -8.410002  ,   8.144573  ,  -8.339226  , ...,  -8.585638  ,\n",
      "          -8.230518  ,   8.409348  ],\n",
      "        ...,\n",
      "        [  5.957677  ,   5.936888  ,  -5.910277  , ...,   6.04119   ,\n",
      "           5.978886  ,  -6.142195  ],\n",
      "        [ -6.329073  ,  -6.4564395 ,  -6.4018745 , ...,   6.3836393 ,\n",
      "          -6.1610155 ,   6.5448585 ],\n",
      "        [  6.8775105 ,   6.55717   ,   6.9502425 , ...,  -7.036024  ,\n",
      "           6.7995634 ,  -6.852325  ]]], dtype=float32)>, <tf.Tensor: shape=(8, 64), dtype=float32, numpy=\n",
      "array([[ 9.5367432e-07,  9.5367432e-07, -6.1988831e-06,  1.9073486e-06,\n",
      "         0.0000000e+00, -2.3841858e-06,  9.5367432e-07,  2.3841858e-06,\n",
      "        -9.5367432e-07,  1.9073486e-06, -2.3841858e-06,  4.7683716e-07,\n",
      "        -1.4305115e-06,  9.5367432e-07,  9.5367432e-07, -3.3378601e-06,\n",
      "        -1.9073486e-06,  1.4305115e-06, -2.3841858e-06,  1.9073486e-06,\n",
      "        -9.5367432e-07,  7.1525574e-06, -3.8146973e-06,  0.0000000e+00,\n",
      "         1.9073486e-06, -9.0599060e-06, -1.4305115e-06,  6.6757202e-06,\n",
      "         4.7683716e-07,  9.5367432e-07, -7.6293945e-06, -3.8146973e-06,\n",
      "        -7.6293945e-06, -1.9073486e-06, -2.3841858e-06,  9.5367432e-07,\n",
      "        -1.4305115e-06,  9.5367432e-07, -2.3841858e-06, -6.6757202e-06,\n",
      "        -7.6293945e-06, -8.5830688e-06,  1.4305115e-06,  5.2452087e-06,\n",
      "         3.3378601e-06, -2.8610229e-06, -3.8146973e-06, -3.3378601e-06,\n",
      "        -9.5367432e-07, -4.7683716e-07, -3.3378601e-06,  4.7683716e-07,\n",
      "         3.8146973e-06, -3.3378601e-06, -2.8610229e-06, -1.9073486e-06,\n",
      "        -1.9073486e-06, -9.5367432e-07, -3.3378601e-06, -1.9073486e-06,\n",
      "         4.2915344e-06,  0.0000000e+00, -1.9073486e-06,  0.0000000e+00],\n",
      "       [-1.9073486e-06, -9.5367432e-07,  1.4305115e-06,  5.7220459e-06,\n",
      "         3.3378601e-06,  2.3841858e-07, -1.4305115e-06,  4.7683716e-07,\n",
      "        -2.3841858e-06,  2.8610229e-06,  4.7683716e-07, -1.1920929e-06,\n",
      "         1.4305115e-06,  0.0000000e+00, -6.1988831e-06, -1.9073486e-06,\n",
      "        -1.6689301e-06, -5.2452087e-06, -1.1920929e-06,  0.0000000e+00,\n",
      "        -1.4305115e-06, -2.3841858e-06,  0.0000000e+00, -4.7683716e-07,\n",
      "         2.8610229e-06,  9.5367432e-07, -1.9073486e-06, -3.8146973e-06,\n",
      "         1.4305115e-06, -2.3841858e-06, -1.6689301e-06, -2.8610229e-06,\n",
      "        -3.5762787e-06,  5.9604645e-06, -1.9073486e-06,  8.1062317e-06,\n",
      "        -5.2452087e-06,  2.6226044e-06, -5.4836273e-06,  9.5367432e-07,\n",
      "        -1.9073486e-06, -1.4305115e-06, -2.3841858e-06,  4.7683716e-06,\n",
      "        -4.7683716e-07,  5.4836273e-06,  1.4305115e-06,  0.0000000e+00,\n",
      "        -3.3378601e-06,  0.0000000e+00,  1.9073486e-06, -2.8610229e-06,\n",
      "         2.6226044e-06,  2.8610229e-06, -2.1457672e-06,  2.3841858e-06,\n",
      "         1.4305115e-06, -9.5367432e-07,  0.0000000e+00,  1.4305115e-06,\n",
      "        -2.3841858e-07,  3.8146973e-06,  4.7683716e-07, -1.4305115e-06],\n",
      "       [ 2.6226044e-06, -2.3841858e-07,  3.8146973e-06,  4.7683716e-07,\n",
      "        -1.1920929e-06,  7.1525574e-07,  3.0994415e-06,  1.9073486e-06,\n",
      "         9.5367432e-07,  1.4305115e-06, -3.8146973e-06, -9.5367432e-07,\n",
      "         4.5299530e-06, -4.7683716e-07, -2.3841858e-07,  2.6226044e-06,\n",
      "         4.0531158e-06, -3.0994415e-06,  1.4305115e-06,  1.4305115e-06,\n",
      "         4.2915344e-06, -9.5367432e-07, -2.1457672e-06, -2.8610229e-06,\n",
      "        -1.4305115e-06,  1.1920929e-06,  4.2915344e-06, -2.3841858e-06,\n",
      "        -1.4305115e-06, -7.1525574e-07, -1.4305115e-06,  2.8610229e-06,\n",
      "        -2.1457672e-06, -3.3378601e-06,  7.1525574e-07,  4.7683716e-07,\n",
      "        -4.7683716e-07, -2.1457672e-06, -1.4305115e-06, -4.7683716e-07,\n",
      "         9.5367432e-07,  4.2915344e-06,  9.5367432e-07,  1.9073486e-06,\n",
      "        -2.1457672e-06, -2.6226044e-06, -2.8610229e-06,  2.3841858e-07,\n",
      "        -4.7683716e-06, -1.4305115e-06, -4.7683716e-07,  9.5367432e-07,\n",
      "        -9.5367432e-07, -2.1457672e-06, -4.7683716e-07, -7.1525574e-07,\n",
      "        -9.5367432e-07, -1.4305115e-06, -2.3841858e-06,  7.1525574e-07,\n",
      "        -1.1920929e-06,  3.8146973e-06,  6.1988831e-06,  1.1920929e-06],\n",
      "       [ 3.8146973e-06, -3.8146973e-06, -1.4305115e-06,  0.0000000e+00,\n",
      "         2.1457672e-06, -1.1920929e-06,  4.7683716e-07,  3.3378601e-06,\n",
      "        -1.4305115e-06, -5.4836273e-06,  4.2915344e-06,  3.3378601e-06,\n",
      "        -9.5367432e-07, -4.7683716e-07,  3.8146973e-06,  2.3841858e-07,\n",
      "        -2.3841858e-06,  3.8146973e-06, -1.4305115e-06,  3.3378601e-06,\n",
      "         2.3841858e-06, -2.3841858e-06,  1.4305115e-06, -4.7683716e-07,\n",
      "         4.2915344e-06, -2.8610229e-06, -5.7220459e-06, -1.9073486e-06,\n",
      "         4.7683716e-07,  2.3841858e-06,  4.5299530e-06, -7.1525574e-07,\n",
      "         1.1920929e-06,  4.7683716e-06, -4.2915344e-06, -4.0531158e-06,\n",
      "        -4.7683716e-06,  2.3841858e-06,  3.5762787e-06,  3.5762787e-06,\n",
      "        -4.7683716e-06,  0.0000000e+00,  1.9073486e-06,  1.9073486e-06,\n",
      "         2.8610229e-06, -3.3378601e-06,  3.0994415e-06, -4.2915344e-06,\n",
      "         0.0000000e+00,  2.3841858e-06, -3.8146973e-06,  5.2452087e-06,\n",
      "        -1.6689301e-06, -9.5367432e-07,  2.3841858e-07,  3.8146973e-06,\n",
      "         1.9073486e-06, -4.0531158e-06,  9.5367432e-07, -1.9073486e-06,\n",
      "        -1.1920929e-06, -1.4305115e-06,  1.9073486e-06,  1.4305115e-06],\n",
      "       [ 0.0000000e+00,  4.2915344e-06,  3.5762787e-06, -1.9073486e-06,\n",
      "        -9.5367432e-07, -2.1457672e-06,  2.6226044e-06, -2.6226044e-06,\n",
      "        -2.6226044e-06, -4.7683716e-07, -3.3378601e-06, -4.7683716e-07,\n",
      "         2.3841858e-06, -1.4305115e-06, -2.3841858e-06, -1.4305115e-06,\n",
      "         1.1920929e-06,  1.9073486e-06,  4.7683716e-07,  0.0000000e+00,\n",
      "         2.8610229e-06,  3.3378601e-06,  1.9073486e-06, -2.3841858e-07,\n",
      "         0.0000000e+00,  4.2915344e-06,  0.0000000e+00,  3.3378601e-06,\n",
      "         1.4305115e-06,  2.8610229e-06, -4.2915344e-06, -1.1920929e-06,\n",
      "         5.0067902e-06,  2.3841858e-07, -3.8146973e-06, -5.9604645e-06,\n",
      "         1.4305115e-06,  2.1457672e-06,  3.8146973e-06, -2.3841858e-06,\n",
      "         3.3378601e-06, -3.8146973e-06,  1.1920929e-06,  7.1525574e-07,\n",
      "         3.0994415e-06, -5.4836273e-06, -4.7683716e-07, -8.1062317e-06,\n",
      "         5.7220459e-06,  5.2452087e-06, -4.0531158e-06,  1.9073486e-06,\n",
      "        -1.4305115e-06,  4.7683716e-07, -5.4836273e-06,  1.1920929e-06,\n",
      "        -2.3841858e-06, -2.1457672e-06, -3.8146973e-06, -1.9073486e-06,\n",
      "        -5.2452087e-06, -3.8146973e-06,  3.3378601e-06, -2.8610229e-06],\n",
      "       [ 2.1457672e-06, -6.9141388e-06,  6.4969063e-06,  1.3113022e-06,\n",
      "         1.9669533e-06, -4.2319298e-06, -6.5565109e-07,  3.0398369e-06,\n",
      "         5.3644180e-07, -4.7683716e-06,  1.4901161e-06,  2.6226044e-06,\n",
      "         3.3974648e-06,  1.7285347e-06,  1.4901161e-06, -1.7881393e-06,\n",
      "        -6.6757202e-06, -3.4570694e-06, -1.6689301e-06,  4.4107437e-06,\n",
      "         2.2649765e-06, -4.7683716e-07,  7.7486038e-07,  1.3113022e-06,\n",
      "        -3.0398369e-06,  4.2915344e-06, -7.1525574e-07,  2.0265579e-06,\n",
      "        -7.7486038e-07,  1.1920929e-06,  7.7486038e-07, -2.6822090e-06,\n",
      "         7.7486038e-07, -2.8014183e-06, -2.2053719e-06,  0.0000000e+00,\n",
      "        -1.6689301e-06,  0.0000000e+00, -5.5432320e-06, -4.2319298e-06,\n",
      "         6.1988831e-06, -2.5033951e-06,  3.1590462e-06,  7.7486038e-07,\n",
      "        -4.4107437e-06,  3.7550926e-06,  1.0728836e-06,  2.3841858e-07,\n",
      "        -6.3776970e-06,  1.5497208e-06, -1.4305115e-06, -1.0132790e-06,\n",
      "        -8.9406967e-07, -2.8014183e-06,  2.8014183e-06, -2.3841858e-07,\n",
      "        -6.7353249e-06, -3.8146973e-06, -2.0265579e-06,  1.5497208e-06,\n",
      "        -2.9802322e-07, -4.5299530e-06,  9.5367432e-07,  0.0000000e+00],\n",
      "       [ 6.5565109e-07,  5.9604645e-07, -4.7087669e-06, -1.1920929e-06,\n",
      "        -2.4437904e-06,  6.1988831e-06, -3.5166740e-06, -1.1920929e-06,\n",
      "        -4.0531158e-06, -7.7486038e-07, -2.1457672e-06,  7.1525574e-07,\n",
      "        -7.1525574e-07, -1.2516975e-06,  1.1324883e-06,  4.7683716e-06,\n",
      "        -5.1259995e-06,  1.5497208e-06, -2.8610229e-06, -3.0994415e-06,\n",
      "        -2.0265579e-06,  3.5762787e-06, -2.9802322e-07, -1.1920929e-07,\n",
      "         3.6954880e-06,  1.1920929e-07,  2.3841858e-06,  2.1457672e-06,\n",
      "         8.7618828e-06,  1.4901161e-06,  5.9604645e-07,  1.5497208e-06,\n",
      "         1.3709068e-06, -1.7881393e-07, -4.8875809e-06,  2.5033951e-06,\n",
      "        -1.9073486e-06, -2.3841858e-06,  5.3644180e-07, -2.3841858e-07,\n",
      "         1.7285347e-06, -3.2186508e-06,  5.3048134e-06,  1.2516975e-06,\n",
      "         4.7683716e-07, -1.7881393e-07,  5.9604645e-08, -1.4305115e-06,\n",
      "        -1.7881393e-06, -3.0994415e-06, -4.2915344e-06, -6.0200691e-06,\n",
      "        -4.6491623e-06,  1.7285347e-06,  2.9206276e-06,  4.1127205e-06,\n",
      "         3.4570694e-06, -3.0398369e-06,  8.3446503e-07, -1.4901161e-06,\n",
      "         4.7683716e-07,  2.2649765e-06,  2.0861626e-06, -1.1920929e-06],\n",
      "       [-3.3378601e-06, -4.2915344e-06,  7.1525574e-07, -9.5367432e-07,\n",
      "        -1.4305115e-06, -9.5367432e-07, -2.3841858e-07, -4.0531158e-06,\n",
      "        -4.2915344e-06, -2.3841858e-06, -7.1525574e-07, -4.7683716e-06,\n",
      "        -2.8610229e-06, -2.8610229e-06, -2.3841858e-06,  1.6689301e-06,\n",
      "        -2.6226044e-06, -2.6226044e-06, -3.3378601e-06, -2.3841858e-06,\n",
      "         1.9073486e-06, -2.3841858e-07,  1.9073486e-06,  0.0000000e+00,\n",
      "        -1.9073486e-06, -4.7683716e-07, -1.4305115e-06,  2.3841858e-06,\n",
      "         3.8146973e-06,  2.3841858e-06, -4.7683716e-06, -4.7683716e-07,\n",
      "        -3.3378601e-06,  2.8610229e-06,  4.5299530e-06, -5.9604645e-06,\n",
      "        -2.3841858e-06, -9.5367432e-07,  2.6226044e-06,  1.4305115e-06,\n",
      "        -2.3841858e-07, -9.5367432e-07,  1.4305115e-06, -4.7683716e-07,\n",
      "        -2.3841858e-07, -1.9073486e-06,  1.9073486e-06,  9.5367432e-07,\n",
      "         4.7683716e-06, -4.2915344e-06,  6.1988831e-06,  1.9073486e-06,\n",
      "        -1.4305115e-06,  2.3841858e-06,  2.3841858e-06,  2.1457672e-06,\n",
      "         4.2915344e-06,  2.8610229e-06, -3.8146973e-06, -4.2915344e-06,\n",
      "         4.7683716e-07, -4.0531158e-06, -2.3841858e-06, -2.3841858e-07]],\n",
      "      dtype=float32)>, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MultiHeadAttention, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# Custom MultiHeadAttention layer that only returns attention_scores\n",
    "class CustomMultiHeadAttention(MultiHeadAttention):\n",
    "    def call(self, query, value, key=None, attention_mask=None, **kwargs):\n",
    "        # Call the original MultiHeadAttention call function\n",
    "        _, attention_scores = super().call(\n",
    "            query=query,\n",
    "            value=value,\n",
    "            key=key,\n",
    "            attention_mask=attention_mask,\n",
    "            return_attention_scores=True,  # Ensure to return attention_scores\n",
    "            **kwargs\n",
    "        )\n",
    "        return attention_scores  # Return only the attention_scores\n",
    "\n",
    "# Build the model\n",
    "input_layer = Input(shape=(5, 64))\n",
    "attention_scores = CustomMultiHeadAttention(num_heads=8, key_dim=64)(input_layer, input_layer)\n",
    "model = Model(inputs=input_layer, outputs=attention_scores)\n",
    "\n",
    "# Compile the model using categorical_crossentropy as the loss function, suitable for probability distribution outputs\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Generate random data as input and target output\n",
    "x_train = np.random.random((100, 5, 64))\n",
    "y_train = np.random.random((100, 8, 5, 5))  # Note: y_train shape should match the expected output shape from the model\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=10)\n",
    "\n",
    "# Use GradientTape to check gradient flow\n",
    "with tf.GradientTape() as tape:\n",
    "    inputs = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "    tape.watch(inputs)\n",
    "    predictions = model(inputs)\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_train, predictions)\n",
    "\n",
    "gradients = tape.gradient(loss, model.trainable_variables)\n",
    "print(\"Gradients on trainable variables:\", gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a4cec8",
   "metadata": {},
   "source": [
    "## MultiHeadAttention predicting delay token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6ed843bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim:  3\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 1s 5ms/step - loss: 3.3770 - accuracy: 0.9000\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.8023 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5.6385e-06 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9a485b5ad0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers, losses\n",
    "import numpy as np\n",
    "\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self, type=\"flows\", l2_reg=0.01):\n",
    "        super(CNN, self).__init__()\n",
    "        self.type = type\n",
    "        if type == \"images_flows\":\n",
    "            dim = 6\n",
    "        elif type == \"cross_flows\":\n",
    "            dim = 9\n",
    "        else:\n",
    "            dim = 3\n",
    "        print(\"dim: \", dim)\n",
    "        self.cnn = models.Sequential()\n",
    "\n",
    "        reg = regularizers.l2(l2_reg)\n",
    "\n",
    "        self.cnn.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, dim), kernel_regularizer=reg))\n",
    "        self.cnn.add(layers.MaxPooling2D((3, 3)))\n",
    "\n",
    "        self.cnn.add(layers.Conv2D(48, (3, 3), activation='relu', kernel_regularizer=reg))\n",
    "        self.cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "        self.cnn.add(layers.Conv2D(48, (3, 3), activation='relu', kernel_regularizer=reg))\n",
    "        self.cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "        self.cnn.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=reg))\n",
    "        self.cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "        self.cnn.add(layers.GlobalAveragePooling2D())\n",
    "        self.cnn.add(layers.Dense(48, activation='relu', kernel_regularizer=reg))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.cnn(inputs)\n",
    "\n",
    "class MLPHead(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim, num_classes, dropout_rate=0.1):\n",
    "        super(MLPHead, self).__init__()\n",
    "        self.dense1 = layers.Dense(hidden_dim, activation='relu')\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.dense2 = layers.Dense(num_classes)\n",
    "        self.softmax = layers.Softmax()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self, num_heads, dff, num_layers, hidden_dim, num_classes, rate=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.class_token = self.add_weight(\"class_token\", shape=[1, 1, 48], initializer=\"random_normal\")\n",
    "        self.encoder_layers = [layers.MultiHeadAttention(num_heads=num_heads, key_dim=48) for _ in range(num_layers)]\n",
    "        self.ffn_layers = [self.point_wise_feed_forward_network(48, dff) for _ in range(num_layers)]\n",
    "        self.dropout_layers = [layers.Dropout(rate) for _ in range(num_layers)]\n",
    "        self.mlp_head = MLPHead(hidden_dim, num_classes, dropout_rate=rate)\n",
    "\n",
    "    def point_wise_feed_forward_network(self, d_model, dff):\n",
    "        return tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        query = inputs['query']\n",
    "        context = inputs['context']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        class_token = tf.broadcast_to(self.class_token, [batch_size, 1, 48])\n",
    "        for i in range(self.num_layers):\n",
    "            class_token = self.encoder_layers[i](query=class_token, value=context, key=context)\n",
    "            class_token = self.ffn_layers[i](class_token)\n",
    "            class_token = self.dropout_layers[i](class_token, training=training)\n",
    "        class_token_output = tf.squeeze(class_token, axis=1)  # Remove the sequence dimension\n",
    "        output = self.mlp_head(class_token_output, training=training)\n",
    "        return output\n",
    "\n",
    "# Instantiate the CNN model\n",
    "Extractor = CNN()\n",
    "\n",
    "# Generate random images\n",
    "random_images = np.random.random((20, 224, 224, 3)) * 255\n",
    "random_images = random_images.astype('float32') / 255.0\n",
    "left_imgs = tf.convert_to_tensor(random_images, dtype=tf.float32)\n",
    "\n",
    "random_images = np.random.random((20, 224, 224, 3)) * 255\n",
    "random_images = random_images.astype('float32') / 255.0\n",
    "right_imgs = tf.convert_to_tensor(random_images, dtype=tf.float32)\n",
    "\n",
    "# Extract features\n",
    "left_features = Extractor(left_imgs)\n",
    "right_features = Extractor(right_imgs)\n",
    "\n",
    "# Reshape features to [num_samples, seq_length, feature_dim]\n",
    "left_features = tf.reshape(left_features, [1, 20, 48])\n",
    "right_features = tf.reshape(right_features, [1, 20, 48])\n",
    "\n",
    "# Prepare dummy labels for training\n",
    "num_classes = 40\n",
    "dummy_labels = tf.random.uniform([1], maxval=num_classes, dtype=tf.int32)  # Single random label\n",
    "\n",
    "# Define the dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({'query': left_features, 'context': right_features}, dummy_labels))\n",
    "dataset = dataset.repeat().batch(1)  # Repeat dataset for training and batch size of 1\n",
    "\n",
    "# Instantiate the Transformer model\n",
    "hidden_dim = 512  # Example hidden dimension for MLP head\n",
    "transformer = TransformerModel(num_heads=8, dff=128, num_layers=2, hidden_dim=hidden_dim, num_classes=num_classes)\n",
    "\n",
    "# Define loss function, optimizer, and metrics\n",
    "loss_fn = losses.SparseCategoricalCrossentropy()\n",
    "optimizer = optimizers.Adam()\n",
    "\n",
    "# Compile the model\n",
    "transformer.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "transformer.fit(dataset, steps_per_epoch=10, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1fb198d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_model_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " multi_head_attention_56 (Mu  multiple                 74928     \n",
      " ltiHeadAttention)                                               \n",
      "                                                                 \n",
      " multi_head_attention_57 (Mu  multiple                 74928     \n",
      " ltiHeadAttention)                                               \n",
      "                                                                 \n",
      " sequential_58 (Sequential)  (None, 1, 48)             12464     \n",
      "                                                                 \n",
      " sequential_59 (Sequential)  (None, 1, 48)             12464     \n",
      "                                                                 \n",
      " dropout_84 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " dropout_85 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " mlp_head_28 (MLPHead)       multiple                  45608     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 220,440\n",
      "Trainable params: 220,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b18f0c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step - loss: 56.6057 - accuracy: 0.0000e+00\n",
      "Test Loss: 56.60567855834961\n",
      "Test Accuracy: 0.0\n",
      "(100, 48)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "[8.7303086e-24 7.5729631e-25 1.7103794e-25 2.8701448e-24 4.6874052e-24\n",
      " 4.4761795e-26 1.9642023e-24 3.7596297e-25 1.9204243e-22 4.6879523e-27\n",
      " 2.6765143e-24 9.7083155e-24 2.8278255e-24 2.5260622e-24 6.4930039e-27\n",
      " 2.8680530e-25 1.2352159e-24 6.6704871e-24 1.0655454e-25 2.7292507e-25\n",
      " 1.5143536e-24 1.2450232e-23 5.3253571e-26 1.5185369e-23 2.6037452e-23\n",
      " 1.0000000e+00 7.3104031e-25 4.6439576e-25 8.1066434e-26 5.5899847e-25\n",
      " 5.5556121e-24 1.1635327e-26 2.4411495e-27 2.2637604e-24 1.5466726e-26\n",
      " 3.5405036e-26 2.5290222e-24 1.0497220e-25 1.7972824e-25 2.4954520e-24]\n"
     ]
    }
   ],
   "source": [
    "# Prepare dummy data for testing\n",
    "test_random_images = np.random.random((20, 224, 224, 3)) * 255\n",
    "test_random_images = test_random_images.astype('float32') / 255.0\n",
    "test_left_imgs = tf.convert_to_tensor(test_random_images, dtype=tf.float32)\n",
    "\n",
    "test_random_images = np.random.random((20, 224, 224, 3)) * 255\n",
    "test_random_images = test_random_images.astype('float32') / 255.0\n",
    "test_right_imgs = tf.convert_to_tensor(test_random_images, dtype=tf.float32)\n",
    "\n",
    "# Extract features for testing\n",
    "test_left_features = Extractor(test_left_imgs)\n",
    "test_right_features = Extractor(test_right_imgs)\n",
    "\n",
    "# Reshape test features to [num_samples, seq_length, feature_dim]\n",
    "test_left_features = tf.reshape(test_left_features, [1, 20, 48])\n",
    "test_right_features = tf.reshape(test_right_features, [1, 20, 48])\n",
    "\n",
    "# Prepare dummy labels for testing\n",
    "test_dummy_labels = tf.random.uniform([1], maxval=num_classes, dtype=tf.int32)  # Single random label\n",
    "\n",
    "# Define the test dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(({'query': test_left_features, 'context': test_right_features}, test_dummy_labels))\n",
    "test_dataset = test_dataset.batch(1)  # Batch size of 1 for testing\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = transformer.evaluate(test_dataset)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Predict new data\n",
    "num_new_samples = 5\n",
    "new_random_images = np.random.random((num_new_samples*20, 224, 224, 3)) * 255\n",
    "new_random_images = new_random_images.astype('float32') / 255.0\n",
    "new_left_imgs = tf.convert_to_tensor(new_random_images, dtype=tf.float32)\n",
    "\n",
    "new_random_images = np.random.random((num_new_samples*20, 224, 224, 3)) * 255\n",
    "new_random_images = new_random_images.astype('float32') / 255.0\n",
    "new_right_imgs = tf.convert_to_tensor(new_random_images, dtype=tf.float32)\n",
    "\n",
    "# Extract features for new data\n",
    "new_left_features = Extractor(new_left_imgs)\n",
    "new_right_features = Extractor(new_right_imgs)\n",
    "print(new_left_features.shape)\n",
    "\n",
    "# Reshape new features to [num_samples, seq_length, feature_dim]\n",
    "new_left_features = tf.reshape(new_left_features, [num_new_samples, 20, 48])\n",
    "new_right_features = tf.reshape(new_right_features, [num_new_samples, 20, 48])\n",
    "\n",
    "# Predict new data\n",
    "predictions = transformer.predict({'query': new_left_features, 'context': new_right_features})\n",
    "print(predictions[0])\n",
    "# predicted_classes = tf.argmax(predictions, axis=-1)\n",
    "# print(f'Predicted classes: {predicted_classes.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fade155",
   "metadata": {},
   "source": [
    "## Training in real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77495d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 48)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "trainDS = np.load(\"/home/jireh/MT/video_sync_v1/trash/featureData.npy\",allow_pickle=True)\n",
    "trainDS[0][0].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3f4a4fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "class MLPHead(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim, num_classes, dropout_rate=0.1):\n",
    "        super(MLPHead, self).__init__()\n",
    "        self.dense1 = layers.Dense(hidden_dim, activation='relu')\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.dense2 = layers.Dense(num_classes)\n",
    "        self.softmax = layers.Softmax()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self, num_heads, dff, num_layers, hidden_dim, num_classes, rate=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.class_token = self.add_weight(\"class_token\", shape=[1, 1, 48], initializer=\"random_normal\")\n",
    "        self.encoder_layers = [layers.MultiHeadAttention(num_heads=num_heads, key_dim=48) for _ in range(num_layers)]\n",
    "        self.ffn_layers = [self.point_wise_feed_forward_network(48, dff) for _ in range(num_layers)]\n",
    "        self.dropout_layers = [layers.Dropout(rate) for _ in range(num_layers)]\n",
    "        self.mlp_head = MLPHead(hidden_dim, num_classes, dropout_rate=rate)\n",
    "\n",
    "    def point_wise_feed_forward_network(self, d_model, dff):\n",
    "        return tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        query = inputs['query']\n",
    "        context = inputs['context']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        class_token = tf.broadcast_to(self.class_token, [batch_size, 1, 48])\n",
    "        for i in range(self.num_layers):\n",
    "            class_token = self.encoder_layers[i](query=class_token, value=context, key=context)\n",
    "            class_token = self.ffn_layers[i](class_token)\n",
    "            class_token = self.dropout_layers[i](class_token, training=training)\n",
    "        class_token_output = tf.squeeze(class_token, axis=1)  # Remove the sequence dimension\n",
    "        output = self.mlp_head(class_token_output, training=training)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c912fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 1s 11ms/step - loss: 3.6896 - accuracy: 0.0273\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 3.6895 - accuracy: 0.0238\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 3.6894 - accuracy: 0.0277\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 3.6897 - accuracy: 0.0191\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 3.6899 - accuracy: 0.0285\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 3.6896 - accuracy: 0.0227\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 3.6894 - accuracy: 0.0227\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 3.6901 - accuracy: 0.0223\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 3.6891 - accuracy: 0.0223\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 3.6898 - accuracy: 0.0230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9a492b1310>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_features = trainDS[:, 0].tolist()\n",
    "right_features = trainDS[:, 1].tolist()\n",
    "y = trainDS[:, 4].tolist()\n",
    "\n",
    "left_features = np.array(left_features)\n",
    "right_features = np.array(right_features)\n",
    "y = np.array(y)\n",
    "\n",
    "left_features = tf.convert_to_tensor(left_features, dtype=tf.float32)\n",
    "right_features = tf.convert_to_tensor(right_features, dtype=tf.float32)\n",
    "y = to_categorical(y,num_classes=40)\n",
    "\n",
    "size = int(len(y) * 0.8)  # 80% of the data for training\n",
    "\n",
    "# left_features_train, left_features_val = left_features[:size], left_features[size:]\n",
    "# right_features_train, right_features_val = right_features[:size], right_features[size:]\n",
    "# y_train, y_val = y[:size], y[size:]\n",
    "\n",
    "# Define the dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({'query': left_features, 'context': right_features}, y))\n",
    "dataset = dataset.repeat().batch(256)  \n",
    "\n",
    "# Instantiate the Transformer model\n",
    "hidden_dim = 512  # Example hidden dimension for MLP head\n",
    "transformer = TransformerModel(num_heads=8, dff=128, num_layers=2, hidden_dim=hidden_dim, num_classes=num_classes)\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "# Compile the model\n",
    "transformer.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "transformer.fit(dataset, steps_per_epoch=10, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5fabc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 08:04:16.991511: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-10 08:04:17.588854: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-10 08:04:18.755148: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.3/lib64:/usr/include/opencv4\n",
      "2024-08-10 08:04:18.755286: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.3/lib64:/usr/include/opencv4\n",
      "2024-08-10 08:04:18.755295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, callbacks\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import f1_score, mean_absolute_error\n",
    "\n",
    "class TransformerDelay:\n",
    "    def __init__(self, num_heads=8, dff=128, num_layers=2, hidden_dim=512, num_classes=40, rate=0.1):\n",
    "        # Initialize the TransformerDelay class with default or custom parameters\n",
    "        self.model = self.build_model(num_heads, dff, num_layers, hidden_dim, num_classes, rate)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), metrics=['accuracy'])\n",
    "\n",
    "    def build_model(self, num_heads, dff, num_layers, hidden_dim, num_classes, rate):\n",
    "        # Inner class to define the MLP Head\n",
    "        class MLPHead(tf.keras.layers.Layer):\n",
    "            def __init__(self, hidden_dim, num_classes, dropout_rate=0.1):\n",
    "                super(MLPHead, self).__init__()\n",
    "                self.dense1 = layers.Dense(hidden_dim, activation='relu')\n",
    "                self.dropout = layers.Dropout(dropout_rate)\n",
    "                self.dense2 = layers.Dense(num_classes)\n",
    "                self.softmax = layers.Softmax()\n",
    "\n",
    "            def call(self, inputs, training=False):\n",
    "                # Define the forward pass for the MLP Head\n",
    "                x = self.dense1(inputs)\n",
    "                x = self.dropout(x, training=training)\n",
    "                x = self.dense2(x)\n",
    "                return self.softmax(x)\n",
    "\n",
    "        # Inner class to define the Transformer Model\n",
    "        class TransformerModel(tf.keras.Model):\n",
    "            def __init__(self, num_heads, dff, num_layers, hidden_dim, num_classes, rate=0.1):\n",
    "                super(TransformerModel, self).__init__()\n",
    "                self.num_layers = num_layers\n",
    "                self.class_token = self.add_weight(\"class_token\", shape=[1, 1, 48], initializer=\"random_normal\")\n",
    "                self.encoder_layers = [layers.MultiHeadAttention(num_heads=num_heads, key_dim=48) for _ in range(num_layers)]\n",
    "                self.ffn_layers = [self.point_wise_feed_forward_network(48, dff) for _ in range(num_layers)]\n",
    "                self.dropout_layers = [layers.Dropout(rate) for _ in range(num_layers)]\n",
    "                self.mlp_head = MLPHead(hidden_dim, num_classes, dropout_rate=rate)\n",
    "\n",
    "            def point_wise_feed_forward_network(self, d_model, dff):\n",
    "                # Define the point-wise feed forward network used in the Transformer layers\n",
    "                return tf.keras.Sequential([\n",
    "                    layers.Dense(dff, activation='relu'),\n",
    "                    layers.Dense(d_model)\n",
    "                ])\n",
    "\n",
    "            def call(self, inputs, training=True):\n",
    "                # Define the forward pass for the Transformer Model\n",
    "                query = inputs['query']\n",
    "                context = inputs['context']\n",
    "                batch_size = tf.shape(query)[0]\n",
    "                class_token = tf.broadcast_to(self.class_token, [batch_size, 1, 48])\n",
    "                for i in range(self.num_layers):\n",
    "                    class_token = self.encoder_layers[i](query=class_token, value=context, key=context)\n",
    "                    class_token = self.ffn_layers[i](class_token)\n",
    "                    class_token = self.dropout_layers[i](class_token, training=training)\n",
    "                class_token_output = tf.squeeze(class_token, axis=1)\n",
    "                output = self.mlp_head(class_token_output, training=training)\n",
    "                return output\n",
    "\n",
    "        # Return the built TransformerModel\n",
    "        return TransformerModel(num_heads, dff, num_layers, hidden_dim, num_classes, rate)\n",
    "\n",
    "    def train(self, pathSimilarityVectorsArray: str, loadweight=\"\", model_name=\"\"):\n",
    "        # Load the training dataset\n",
    "        trainDS = np.load(fr\"{pathSimilarityVectorsArray}\", allow_pickle=True)\n",
    "        left_features = trainDS[:, 0].tolist()\n",
    "        right_features = trainDS[:, 1].tolist()\n",
    "        y = trainDS[:, 4].tolist()\n",
    "\n",
    "        # Convert data to the appropriate format\n",
    "        left_features = np.array(left_features)\n",
    "        right_features = np.array(right_features)\n",
    "        y = np.array(y)\n",
    "\n",
    "        left_features = tf.convert_to_tensor(left_features, dtype=tf.float32)\n",
    "        right_features = tf.convert_to_tensor(right_features, dtype=tf.float32)\n",
    "        y = to_categorical(y, num_classes=40)\n",
    "\n",
    "        # Split the dataset into training and validation sets\n",
    "        size = int(len(y) * 0.8)\n",
    "        left_features_train, left_features_val = left_features[:size], left_features[size:]\n",
    "        right_features_train, right_features_val = right_features[:size], right_features[size:]\n",
    "        y_train, y_val = y[:size], y[size:]\n",
    "\n",
    "        # Create the TensorFlow dataset for training\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            ({'query': left_features_train, 'context': right_features_train}, y_train)\n",
    "        ).repeat().batch(256)\n",
    "\n",
    "        # Define the directory for saving model weights\n",
    "        weightsPath = fr\"model/TransformerDelay/weight/{model_name}\"\n",
    "        if not os.path.exists(weightsPath):\n",
    "            os.makedirs(weightsPath)\n",
    "\n",
    "        # Define early stopping callback and model checkpoint callback\n",
    "        best_metrics_callback = callbacks.EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "        checkPoint = callbacks.ModelCheckpoint(\n",
    "            filepath=fr\"{weightsPath}/weights\",\n",
    "            save_weights_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=True)\n",
    "\n",
    "        # Load pre-trained weights if provided\n",
    "        if loadweight:\n",
    "            try:\n",
    "                self.loadWeights(loadweight)\n",
    "            except Exception as e:\n",
    "                print(f\"Missing weight file\")\n",
    "\n",
    "        # Train the model\n",
    "        self.model.fit(dataset, validation_data=({'query': left_features_val, 'context': right_features_val}, y_val),\n",
    "                       callbacks=[checkPoint, best_metrics_callback], steps_per_epoch=10, epochs=200)\n",
    "        self.model.load_weights(fr\"{weightsPath}/weights\")\n",
    "\n",
    "    def loadWeights(self, weightsPath):\n",
    "        # Load model weights from the specified path\n",
    "        self.model.load_weights(weightsPath).expect_partial()\n",
    "\n",
    "    def evaluate(self, pathSimilarityVectorsArray, loadweight=\"\"):\n",
    "        # Load the evaluation dataset\n",
    "        print(fr\"{pathSimilarityVectorsArray}\")\n",
    "        db = np.load(fr\"{pathSimilarityVectorsArray}\", allow_pickle=True)\n",
    "\n",
    "        left_features = db[:, 0].tolist()\n",
    "        right_features = db[:, 1].tolist()\n",
    "        y = db[:, 4].tolist()\n",
    "\n",
    "        # Convert data to the appropriate format\n",
    "        left_features = np.array(left_features)\n",
    "        right_features = np.array(right_features)\n",
    "        y = np.array(y)\n",
    "\n",
    "        left_features = tf.convert_to_tensor(left_features, dtype=tf.float32)\n",
    "        right_features = tf.convert_to_tensor(right_features, dtype=tf.float32)\n",
    "        y = to_categorical(y, num_classes=40)\n",
    "\n",
    "        # Load pre-trained weights if provided\n",
    "        if loadweight:\n",
    "            try:\n",
    "                self.loadWeights(loadweight)\n",
    "            except Exception as e:\n",
    "                print(f\"Missing weight file\")\n",
    "\n",
    "        # Evaluate the model\n",
    "        self.model.evaluate({'query': left_features, 'context': right_features}, y)\n",
    "\n",
    "    def predict(self, pathSimilarityVectorsArray, loadweight=\"\", visualization=False):\n",
    "        # Load the prediction dataset\n",
    "        db = np.load(fr\"{pathSimilarityVectorsArray}\", allow_pickle=True)\n",
    "        left_features = db[:, 0].tolist()\n",
    "        right_features = db[:, 1].tolist()\n",
    "        y = db[:, 4].tolist()\n",
    "\n",
    "        # Convert data to the appropriate format\n",
    "        left_features = np.array(left_features)\n",
    "        right_features = np.array(right_features)\n",
    "\n",
    "        left_features = tf.convert_to_tensor(left_features, dtype=tf.float32)\n",
    "        right_features = tf.convert_to_tensor(right_features, dtype=tf.float32)\n",
    "\n",
    "        # Load pre-trained weights if provided\n",
    "        if loadweight:\n",
    "            try:\n",
    "                self.loadWeights(loadweight)\n",
    "            except Exception as e:\n",
    "                print(f\"Missing weight file\")\n",
    "\n",
    "        # Make predictions using the model\n",
    "        output = self.model({'query': left_features, 'context': right_features})\n",
    "        output = tf.argmax(output, axis=1).numpy()\n",
    "\n",
    "        # Display prediction results\n",
    "        print(f\"Predicted values (30 samples): {output[:30]}\")\n",
    "        print(f\"Actual labels (30 samples): {y[:30]}\")\n",
    "        print(f\"Overall accuracy: {np.sum(output == y) / len(y)}\")\n",
    "        print(f\"F1 score: {f1_score(y, output, average='weighted')}\")\n",
    "        print(f\"Mean absolute error: {mean_absolute_error(y, output)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "530a844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 08:04:21.322391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-08-10 08:04:21.351851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-08-10 08:04:21.352102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-08-10 08:04:21.353481: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-10 08:04:21.354236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-08-10 08:04:21.354564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-08-10 08:04:21.354799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-08-10 08:04:22.318234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-08-10 08:04:22.318447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-08-10 08:04:22.318601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-08-10 08:04:22.318762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2578 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10/10 [==============================] - 3s 84ms/step - loss: 3.6886 - accuracy: 0.0281 - val_loss: 3.6893 - val_accuracy: 0.0265\n",
      "Epoch 2/200\n",
      "10/10 [==============================] - 0s 35ms/step - loss: 3.6891 - accuracy: 0.0242 - val_loss: 3.6893 - val_accuracy: 0.0255\n",
      "Epoch 3/200\n",
      "10/10 [==============================] - 0s 34ms/step - loss: 3.6896 - accuracy: 0.0258 - val_loss: 3.6893 - val_accuracy: 0.0265\n",
      "Epoch 4/200\n",
      "10/10 [==============================] - 0s 34ms/step - loss: 3.6883 - accuracy: 0.0242 - val_loss: 3.6893 - val_accuracy: 0.0260\n",
      "Epoch 5/200\n",
      "10/10 [==============================] - 0s 34ms/step - loss: 3.6893 - accuracy: 0.0258 - val_loss: 3.6893 - val_accuracy: 0.0260\n",
      "Epoch 6/200\n",
      "10/10 [==============================] - 0s 34ms/step - loss: 3.6893 - accuracy: 0.0250 - val_loss: 3.6892 - val_accuracy: 0.0255\n",
      "Epoch 7/200\n",
      "10/10 [==============================] - 0s 34ms/step - loss: 3.6882 - accuracy: 0.0215 - val_loss: 3.6892 - val_accuracy: 0.0255\n",
      "Epoch 8/200\n",
      "10/10 [==============================] - 0s 34ms/step - loss: 3.6891 - accuracy: 0.0250 - val_loss: 3.6892 - val_accuracy: 0.0255\n",
      "Epoch 9/200\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 3.6892 - accuracy: 0.0242 - val_loss: 3.6892 - val_accuracy: 0.0255\n",
      "Epoch 10/200\n",
      "10/10 [==============================] - 0s 33ms/step - loss: 3.6882 - accuracy: 0.0270 - val_loss: 3.6892 - val_accuracy: 0.0255\n",
      "Epoch 11/200\n",
      "10/10 [==============================] - 0s 34ms/step - loss: 3.6889 - accuracy: 0.0219 - val_loss: 3.6892 - val_accuracy: 0.0255\n",
      "/home/jireh/MT/video_sync_v1/trash/featureData.npy\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 3.6891 - accuracy: 0.0250\n",
      "Predicted values (30 samples): [30  7  7 30 30 30 30 30  7 30 30 34 30 30 30 33 30 30  7 30 30 30 30  7\n",
      " 30 30 30 30 30 30]\n",
      "Actual labels (30 samples): [20, 9, 4, 13, -12, 1, -11, -2, -1, 10, 14, -13, -17, 5, 15, -19, 6, 2, -13, -9, -18, 1, -5, 0, 12, -12, 20, -18, -15, 7]\n",
      "Overall accuracy: 0.0066\n",
      "F1 score: 0.0012637071420341381\n",
      "Mean absolute error: 24.4104\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create an instance of TransformerDelay\n",
    "transformer_delay = TransformerDelay(num_heads=8, dff=128, num_layers=2, hidden_dim=512, num_classes=40, rate=0.01)\n",
    "\n",
    "# Step 2: Train the model\n",
    "transformer_delay.train(\n",
    "    pathSimilarityVectorsArray=\"/home/jireh/MT/video_sync_v1/trash/featureData.npy\", \n",
    "    loadweight=\"\", \n",
    "    model_name=\"my_transformer_model\"\n",
    ")\n",
    "\n",
    "# Step 3: Evaluate the model\n",
    "transformer_delay.evaluate(\n",
    "    pathSimilarityVectorsArray=\"/home/jireh/MT/video_sync_v1/trash/featureData.npy\",\n",
    "    loadweight=\"model/TransformerDelay/weight/my_transformer_model/weights\"  # Load the trained weights\n",
    ")\n",
    "\n",
    "# Step 4: Make predictions\n",
    "transformer_delay.predict(\n",
    "    pathSimilarityVectorsArray=\"/home/jireh/MT/video_sync_v1/trash/featureData.npy\",\n",
    "    loadweight=\"model/TransformerDelay/weight/my_transformer_model/weights\",  # Load the trained weights\n",
    "    visualization=False  # Set to True to enable custom visualization\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad4152d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfgpu]",
   "language": "python",
   "name": "conda-env-tfgpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
